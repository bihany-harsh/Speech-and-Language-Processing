{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ffn_lm](./images/ffn_class.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO BUILD A FEEDFORWARD NEURAL NETWORK FOR FORWARD INFERENCE AND TEXT GENERATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "# preprocess text\n",
    "import re\n",
    "\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "tokens = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208530,\n",
       " ['first',\n",
       "  'citizen',\n",
       "  'before',\n",
       "  'we',\n",
       "  'proceed',\n",
       "  'any',\n",
       "  'further',\n",
       "  'hear',\n",
       "  'me',\n",
       "  'speak'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens), tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(tokens)\n",
    "\n",
    "word_2_index = {word: index for index, word in enumerate(word_count.keys())}\n",
    "index_to_word = {index: word for word, index in word_2_index.items()}\n",
    "\n",
    "vocab_size = len(word_2_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11456\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset with a sequence_length as the context (window size)\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, tokens, word_2_index, sequence_length):\n",
    "        self.tokens = tokens\n",
    "        self.word_2_index = word_2_index\n",
    "        self.sequence_length = sequence_length\n",
    "        self.int_text = [\n",
    "            self.word_2_index[word] for word in self.tokens if word in self.word_2_index\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.int_text) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            # self.int_text[index : index + self.sequence_length],\n",
    "            # self.int_text[index + self.sequence_length],\n",
    "            torch.tensor(self.int_text[index : index + self.sequence_length], dtype=torch.long),\n",
    "            torch.tensor(self.int_text[index + self.sequence_length], dtype=torch.long),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 103, 3502,   32,  490, 3621]), tensor(217))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the TextDataset class\n",
    "dataset = TextDataset(tokens, word_2_index, 5)\n",
    "dataset[26569]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A feedforward model for text generation\n",
    "class FFn_LM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        embeds = embeds.mean(dim=1)\n",
    "        out = F.relu(self.fc1(embeds))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train, test_token = train_test_split(tokens, test_size=0.2)\n",
    "\n",
    "# building the dataloader\n",
    "sequence_length = 8\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TextDataset(tokens_train, word_2_index, sequence_length)\n",
    "test_dataset = TextDataset(test_token, word_2_index, sequence_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the model\n",
    "embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "output_dim = vocab_size\n",
    "\n",
    "model = FFn_LM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Batch: 100, Loss: 6.8662\n",
      "Epoch: 1, Batch: 200, Loss: 6.9605\n",
      "Epoch: 1, Batch: 300, Loss: 6.5465\n",
      "Epoch: 1, Batch: 400, Loss: 6.0991\n",
      "Epoch: 1, Batch: 500, Loss: 6.5380\n",
      "Epoch: 1, Batch: 600, Loss: 6.5286\n",
      "Epoch: 1, Batch: 700, Loss: 6.4445\n",
      "Epoch: 1, Batch: 800, Loss: 6.3816\n",
      "Epoch: 1, Batch: 900, Loss: 6.4270\n",
      "Epoch: 1, Batch: 1000, Loss: 6.4918\n",
      "Epoch: 1, Batch: 1100, Loss: 6.8839\n",
      "Epoch: 1, Batch: 1200, Loss: 7.2240\n",
      "Epoch: 1, Batch: 1300, Loss: 6.5994\n",
      "Epoch: 1, Batch: 1400, Loss: 6.7963\n",
      "Epoch: 1, Batch: 1500, Loss: 6.4626\n",
      "Epoch: 1, Batch: 1600, Loss: 6.3890\n",
      "Epoch: 1, Batch: 1700, Loss: 6.7548\n",
      "Epoch: 1, Batch: 1800, Loss: 6.1394\n",
      "Epoch: 1, Batch: 1900, Loss: 6.5871\n",
      "Epoch: 1, Batch: 2000, Loss: 6.5121\n",
      "Epoch: 1, Batch: 2100, Loss: 6.2538\n",
      "Epoch: 1, Batch: 2200, Loss: 6.6523\n",
      "Epoch: 1, Batch: 2300, Loss: 6.5359\n",
      "Epoch: 1, Batch: 2400, Loss: 6.7929\n",
      "Epoch: 1, Batch: 2500, Loss: 6.2310\n",
      "Epoch: 1, Batch: 2600, Loss: 7.1097\n",
      "Epoch: 2, Batch: 100, Loss: 5.9863\n",
      "Epoch: 2, Batch: 200, Loss: 6.3892\n",
      "Epoch: 2, Batch: 300, Loss: 6.4596\n",
      "Epoch: 2, Batch: 400, Loss: 6.3518\n",
      "Epoch: 2, Batch: 500, Loss: 5.9267\n",
      "Epoch: 2, Batch: 600, Loss: 6.2051\n",
      "Epoch: 2, Batch: 700, Loss: 5.9600\n",
      "Epoch: 2, Batch: 800, Loss: 6.2648\n",
      "Epoch: 2, Batch: 900, Loss: 6.4982\n",
      "Epoch: 2, Batch: 1000, Loss: 6.4605\n",
      "Epoch: 2, Batch: 1100, Loss: 5.7139\n",
      "Epoch: 2, Batch: 1200, Loss: 6.4260\n",
      "Epoch: 2, Batch: 1300, Loss: 6.2312\n",
      "Epoch: 2, Batch: 1400, Loss: 6.2858\n",
      "Epoch: 2, Batch: 1500, Loss: 6.6332\n",
      "Epoch: 2, Batch: 1600, Loss: 6.2974\n",
      "Epoch: 2, Batch: 1700, Loss: 6.0102\n",
      "Epoch: 2, Batch: 1800, Loss: 6.1567\n",
      "Epoch: 2, Batch: 1900, Loss: 6.3201\n",
      "Epoch: 2, Batch: 2000, Loss: 6.2359\n",
      "Epoch: 2, Batch: 2100, Loss: 6.2666\n",
      "Epoch: 2, Batch: 2200, Loss: 5.8890\n",
      "Epoch: 2, Batch: 2300, Loss: 6.3856\n",
      "Epoch: 2, Batch: 2400, Loss: 6.1774\n",
      "Epoch: 2, Batch: 2500, Loss: 6.3900\n",
      "Epoch: 2, Batch: 2600, Loss: 6.5926\n",
      "Epoch: 3, Batch: 100, Loss: 5.8924\n",
      "Epoch: 3, Batch: 200, Loss: 5.8628\n",
      "Epoch: 3, Batch: 300, Loss: 5.9705\n",
      "Epoch: 3, Batch: 400, Loss: 6.0538\n",
      "Epoch: 3, Batch: 500, Loss: 6.2992\n",
      "Epoch: 3, Batch: 600, Loss: 6.1509\n",
      "Epoch: 3, Batch: 700, Loss: 6.3039\n",
      "Epoch: 3, Batch: 800, Loss: 6.1159\n",
      "Epoch: 3, Batch: 900, Loss: 5.8997\n",
      "Epoch: 3, Batch: 1000, Loss: 6.2826\n",
      "Epoch: 3, Batch: 1100, Loss: 5.7250\n",
      "Epoch: 3, Batch: 1200, Loss: 6.0844\n",
      "Epoch: 3, Batch: 1300, Loss: 5.5045\n",
      "Epoch: 3, Batch: 1400, Loss: 6.0324\n",
      "Epoch: 3, Batch: 1500, Loss: 5.6233\n",
      "Epoch: 3, Batch: 1600, Loss: 5.9207\n",
      "Epoch: 3, Batch: 1700, Loss: 5.9010\n",
      "Epoch: 3, Batch: 1800, Loss: 6.1281\n",
      "Epoch: 3, Batch: 1900, Loss: 5.7232\n",
      "Epoch: 3, Batch: 2000, Loss: 5.9631\n",
      "Epoch: 3, Batch: 2100, Loss: 6.2287\n",
      "Epoch: 3, Batch: 2200, Loss: 6.0755\n",
      "Epoch: 3, Batch: 2300, Loss: 5.9136\n",
      "Epoch: 3, Batch: 2400, Loss: 5.7531\n",
      "Epoch: 3, Batch: 2500, Loss: 5.8102\n",
      "Epoch: 3, Batch: 2600, Loss: 5.9054\n",
      "Epoch: 4, Batch: 100, Loss: 5.8517\n",
      "Epoch: 4, Batch: 200, Loss: 5.4582\n",
      "Epoch: 4, Batch: 300, Loss: 5.8214\n",
      "Epoch: 4, Batch: 400, Loss: 5.7260\n",
      "Epoch: 4, Batch: 500, Loss: 5.5614\n",
      "Epoch: 4, Batch: 600, Loss: 5.7725\n",
      "Epoch: 4, Batch: 700, Loss: 5.5739\n",
      "Epoch: 4, Batch: 800, Loss: 5.8487\n",
      "Epoch: 4, Batch: 900, Loss: 5.7033\n",
      "Epoch: 4, Batch: 1000, Loss: 5.6886\n",
      "Epoch: 4, Batch: 1100, Loss: 5.5402\n",
      "Epoch: 4, Batch: 1200, Loss: 5.7355\n",
      "Epoch: 4, Batch: 1300, Loss: 5.5464\n",
      "Epoch: 4, Batch: 1400, Loss: 5.8154\n",
      "Epoch: 4, Batch: 1500, Loss: 5.4211\n",
      "Epoch: 4, Batch: 1600, Loss: 5.5764\n",
      "Epoch: 4, Batch: 1700, Loss: 5.4335\n",
      "Epoch: 4, Batch: 1800, Loss: 5.9517\n",
      "Epoch: 4, Batch: 1900, Loss: 5.9366\n",
      "Epoch: 4, Batch: 2000, Loss: 5.7746\n",
      "Epoch: 4, Batch: 2100, Loss: 5.5071\n",
      "Epoch: 4, Batch: 2200, Loss: 5.4362\n",
      "Epoch: 4, Batch: 2300, Loss: 5.6103\n",
      "Epoch: 4, Batch: 2400, Loss: 6.0736\n",
      "Epoch: 4, Batch: 2500, Loss: 5.6058\n",
      "Epoch: 4, Batch: 2600, Loss: 5.5180\n",
      "Epoch: 5, Batch: 100, Loss: 5.3502\n",
      "Epoch: 5, Batch: 200, Loss: 5.1423\n",
      "Epoch: 5, Batch: 300, Loss: 5.3491\n",
      "Epoch: 5, Batch: 400, Loss: 5.0201\n",
      "Epoch: 5, Batch: 500, Loss: 5.3246\n",
      "Epoch: 5, Batch: 600, Loss: 5.2000\n",
      "Epoch: 5, Batch: 700, Loss: 5.0126\n",
      "Epoch: 5, Batch: 800, Loss: 4.9971\n",
      "Epoch: 5, Batch: 900, Loss: 5.2283\n",
      "Epoch: 5, Batch: 1000, Loss: 5.2263\n",
      "Epoch: 5, Batch: 1100, Loss: 5.1470\n",
      "Epoch: 5, Batch: 1200, Loss: 5.0685\n",
      "Epoch: 5, Batch: 1300, Loss: 5.3022\n",
      "Epoch: 5, Batch: 1400, Loss: 5.5815\n",
      "Epoch: 5, Batch: 1500, Loss: 5.3765\n",
      "Epoch: 5, Batch: 1600, Loss: 5.2631\n",
      "Epoch: 5, Batch: 1700, Loss: 5.2071\n",
      "Epoch: 5, Batch: 1800, Loss: 5.1661\n",
      "Epoch: 5, Batch: 1900, Loss: 4.7534\n",
      "Epoch: 5, Batch: 2000, Loss: 4.8460\n",
      "Epoch: 5, Batch: 2100, Loss: 5.1410\n",
      "Epoch: 5, Batch: 2200, Loss: 4.9499\n",
      "Epoch: 5, Batch: 2300, Loss: 5.0855\n",
      "Epoch: 5, Batch: 2400, Loss: 5.0557\n",
      "Epoch: 5, Batch: 2500, Loss: 5.1081\n",
      "Epoch: 5, Batch: 2600, Loss: 4.9950\n",
      "Epoch: 6, Batch: 100, Loss: 4.3247\n",
      "Epoch: 6, Batch: 200, Loss: 4.5963\n",
      "Epoch: 6, Batch: 300, Loss: 4.5630\n",
      "Epoch: 6, Batch: 400, Loss: 4.5430\n",
      "Epoch: 6, Batch: 500, Loss: 4.7348\n",
      "Epoch: 6, Batch: 600, Loss: 4.8932\n",
      "Epoch: 6, Batch: 700, Loss: 4.6678\n",
      "Epoch: 6, Batch: 800, Loss: 4.4320\n",
      "Epoch: 6, Batch: 900, Loss: 4.2442\n",
      "Epoch: 6, Batch: 1000, Loss: 4.4923\n",
      "Epoch: 6, Batch: 1100, Loss: 4.4677\n",
      "Epoch: 6, Batch: 1200, Loss: 4.4018\n",
      "Epoch: 6, Batch: 1300, Loss: 4.4157\n",
      "Epoch: 6, Batch: 1400, Loss: 4.2941\n",
      "Epoch: 6, Batch: 1500, Loss: 4.7369\n",
      "Epoch: 6, Batch: 1600, Loss: 4.8369\n",
      "Epoch: 6, Batch: 1700, Loss: 4.7702\n",
      "Epoch: 6, Batch: 1800, Loss: 4.7499\n",
      "Epoch: 6, Batch: 1900, Loss: 4.5965\n",
      "Epoch: 6, Batch: 2000, Loss: 4.4007\n",
      "Epoch: 6, Batch: 2100, Loss: 4.4833\n",
      "Epoch: 6, Batch: 2200, Loss: 4.4664\n",
      "Epoch: 6, Batch: 2300, Loss: 4.1831\n",
      "Epoch: 6, Batch: 2400, Loss: 4.5382\n",
      "Epoch: 6, Batch: 2500, Loss: 4.6186\n",
      "Epoch: 6, Batch: 2600, Loss: 4.7552\n",
      "Epoch: 7, Batch: 100, Loss: 3.9556\n",
      "Epoch: 7, Batch: 200, Loss: 3.6539\n",
      "Epoch: 7, Batch: 300, Loss: 4.0560\n",
      "Epoch: 7, Batch: 400, Loss: 3.8951\n",
      "Epoch: 7, Batch: 500, Loss: 3.9867\n",
      "Epoch: 7, Batch: 600, Loss: 3.9153\n",
      "Epoch: 7, Batch: 700, Loss: 3.9291\n",
      "Epoch: 7, Batch: 800, Loss: 4.3163\n",
      "Epoch: 7, Batch: 900, Loss: 4.2012\n",
      "Epoch: 7, Batch: 1000, Loss: 3.9137\n",
      "Epoch: 7, Batch: 1100, Loss: 4.1045\n",
      "Epoch: 7, Batch: 1200, Loss: 4.4719\n",
      "Epoch: 7, Batch: 1300, Loss: 4.4183\n",
      "Epoch: 7, Batch: 1400, Loss: 4.0689\n",
      "Epoch: 7, Batch: 1500, Loss: 3.7623\n",
      "Epoch: 7, Batch: 1600, Loss: 4.2177\n",
      "Epoch: 7, Batch: 1700, Loss: 3.2893\n",
      "Epoch: 7, Batch: 1800, Loss: 3.7304\n",
      "Epoch: 7, Batch: 1900, Loss: 3.9503\n",
      "Epoch: 7, Batch: 2000, Loss: 3.6650\n",
      "Epoch: 7, Batch: 2100, Loss: 4.1391\n",
      "Epoch: 7, Batch: 2200, Loss: 4.1826\n",
      "Epoch: 7, Batch: 2300, Loss: 3.8783\n",
      "Epoch: 7, Batch: 2400, Loss: 4.5107\n",
      "Epoch: 7, Batch: 2500, Loss: 4.6183\n",
      "Epoch: 7, Batch: 2600, Loss: 4.3546\n",
      "Epoch: 8, Batch: 100, Loss: 3.4088\n",
      "Epoch: 8, Batch: 200, Loss: 3.5574\n",
      "Epoch: 8, Batch: 300, Loss: 3.7755\n",
      "Epoch: 8, Batch: 400, Loss: 3.9157\n",
      "Epoch: 8, Batch: 500, Loss: 3.1224\n",
      "Epoch: 8, Batch: 600, Loss: 3.4655\n",
      "Epoch: 8, Batch: 700, Loss: 3.5585\n",
      "Epoch: 8, Batch: 800, Loss: 3.8564\n",
      "Epoch: 8, Batch: 900, Loss: 3.2622\n",
      "Epoch: 8, Batch: 1000, Loss: 3.6458\n",
      "Epoch: 8, Batch: 1100, Loss: 3.3462\n",
      "Epoch: 8, Batch: 1200, Loss: 3.2837\n",
      "Epoch: 8, Batch: 1300, Loss: 3.3991\n",
      "Epoch: 8, Batch: 1400, Loss: 3.8501\n",
      "Epoch: 8, Batch: 1500, Loss: 3.3846\n",
      "Epoch: 8, Batch: 1600, Loss: 3.8933\n",
      "Epoch: 8, Batch: 1700, Loss: 3.3995\n",
      "Epoch: 8, Batch: 1800, Loss: 4.0476\n",
      "Epoch: 8, Batch: 1900, Loss: 3.9382\n",
      "Epoch: 8, Batch: 2000, Loss: 3.4909\n",
      "Epoch: 8, Batch: 2100, Loss: 3.8369\n",
      "Epoch: 8, Batch: 2200, Loss: 3.6457\n",
      "Epoch: 8, Batch: 2300, Loss: 4.0075\n",
      "Epoch: 8, Batch: 2400, Loss: 3.3658\n",
      "Epoch: 8, Batch: 2500, Loss: 3.8487\n",
      "Epoch: 8, Batch: 2600, Loss: 3.5120\n",
      "Epoch: 9, Batch: 100, Loss: 3.2989\n",
      "Epoch: 9, Batch: 200, Loss: 3.6125\n",
      "Epoch: 9, Batch: 300, Loss: 2.9823\n",
      "Epoch: 9, Batch: 400, Loss: 3.6349\n",
      "Epoch: 9, Batch: 500, Loss: 3.3540\n",
      "Epoch: 9, Batch: 600, Loss: 3.5228\n",
      "Epoch: 9, Batch: 700, Loss: 3.1619\n",
      "Epoch: 9, Batch: 800, Loss: 3.3701\n",
      "Epoch: 9, Batch: 900, Loss: 3.3252\n",
      "Epoch: 9, Batch: 1000, Loss: 3.4213\n",
      "Epoch: 9, Batch: 1100, Loss: 3.0438\n",
      "Epoch: 9, Batch: 1200, Loss: 3.3784\n",
      "Epoch: 9, Batch: 1300, Loss: 2.8235\n",
      "Epoch: 9, Batch: 1400, Loss: 3.7592\n",
      "Epoch: 9, Batch: 1500, Loss: 2.9558\n",
      "Epoch: 9, Batch: 1600, Loss: 3.5277\n",
      "Epoch: 9, Batch: 1700, Loss: 3.3431\n",
      "Epoch: 9, Batch: 1800, Loss: 3.4400\n",
      "Epoch: 9, Batch: 1900, Loss: 3.4982\n",
      "Epoch: 9, Batch: 2000, Loss: 3.2141\n",
      "Epoch: 9, Batch: 2100, Loss: 3.1402\n",
      "Epoch: 9, Batch: 2200, Loss: 3.3467\n",
      "Epoch: 9, Batch: 2300, Loss: 3.7260\n",
      "Epoch: 9, Batch: 2400, Loss: 3.3183\n",
      "Epoch: 9, Batch: 2500, Loss: 3.4866\n",
      "Epoch: 9, Batch: 2600, Loss: 3.3794\n",
      "Epoch: 10, Batch: 100, Loss: 2.6866\n",
      "Epoch: 10, Batch: 200, Loss: 2.7220\n",
      "Epoch: 10, Batch: 300, Loss: 3.3779\n",
      "Epoch: 10, Batch: 400, Loss: 2.9922\n",
      "Epoch: 10, Batch: 500, Loss: 3.3663\n",
      "Epoch: 10, Batch: 600, Loss: 3.0849\n",
      "Epoch: 10, Batch: 700, Loss: 2.6904\n",
      "Epoch: 10, Batch: 800, Loss: 3.1704\n",
      "Epoch: 10, Batch: 900, Loss: 3.1074\n",
      "Epoch: 10, Batch: 1000, Loss: 3.3667\n",
      "Epoch: 10, Batch: 1100, Loss: 2.4534\n",
      "Epoch: 10, Batch: 1200, Loss: 2.8213\n",
      "Epoch: 10, Batch: 1300, Loss: 3.1838\n",
      "Epoch: 10, Batch: 1400, Loss: 3.0581\n",
      "Epoch: 10, Batch: 1500, Loss: 3.2220\n",
      "Epoch: 10, Batch: 1600, Loss: 2.9642\n",
      "Epoch: 10, Batch: 1700, Loss: 3.3879\n",
      "Epoch: 10, Batch: 1800, Loss: 3.1326\n",
      "Epoch: 10, Batch: 1900, Loss: 3.0592\n",
      "Epoch: 10, Batch: 2000, Loss: 3.1006\n",
      "Epoch: 10, Batch: 2100, Loss: 3.0750\n",
      "Epoch: 10, Batch: 2200, Loss: 3.1118\n",
      "Epoch: 10, Batch: 2300, Loss: 3.5502\n",
      "Epoch: 10, Batch: 2400, Loss: 3.2748\n",
      "Epoch: 10, Batch: 2500, Loss: 3.1812\n",
      "Epoch: 10, Batch: 2600, Loss: 3.5480\n",
      "Epoch: 11, Batch: 100, Loss: 3.2320\n",
      "Epoch: 11, Batch: 200, Loss: 2.7135\n",
      "Epoch: 11, Batch: 300, Loss: 2.7500\n",
      "Epoch: 11, Batch: 400, Loss: 2.8970\n",
      "Epoch: 11, Batch: 500, Loss: 2.8144\n",
      "Epoch: 11, Batch: 600, Loss: 2.8725\n",
      "Epoch: 11, Batch: 700, Loss: 2.6380\n",
      "Epoch: 11, Batch: 800, Loss: 2.7929\n",
      "Epoch: 11, Batch: 900, Loss: 2.9662\n",
      "Epoch: 11, Batch: 1000, Loss: 2.7849\n",
      "Epoch: 11, Batch: 1100, Loss: 2.4464\n",
      "Epoch: 11, Batch: 1200, Loss: 2.3084\n",
      "Epoch: 11, Batch: 1300, Loss: 2.3825\n",
      "Epoch: 11, Batch: 1400, Loss: 3.3149\n",
      "Epoch: 11, Batch: 1500, Loss: 3.2509\n",
      "Epoch: 11, Batch: 1600, Loss: 3.0603\n",
      "Epoch: 11, Batch: 1700, Loss: 2.8334\n",
      "Epoch: 11, Batch: 1800, Loss: 2.5233\n",
      "Epoch: 11, Batch: 1900, Loss: 3.2279\n",
      "Epoch: 11, Batch: 2000, Loss: 2.9320\n",
      "Epoch: 11, Batch: 2100, Loss: 3.1960\n",
      "Epoch: 11, Batch: 2200, Loss: 2.6142\n",
      "Epoch: 11, Batch: 2300, Loss: 3.0006\n",
      "Epoch: 11, Batch: 2400, Loss: 2.2835\n",
      "Epoch: 11, Batch: 2500, Loss: 3.0934\n",
      "Epoch: 11, Batch: 2600, Loss: 3.3317\n",
      "Epoch: 12, Batch: 100, Loss: 2.7114\n",
      "Epoch: 12, Batch: 200, Loss: 2.4738\n",
      "Epoch: 12, Batch: 300, Loss: 2.5212\n",
      "Epoch: 12, Batch: 400, Loss: 2.5292\n",
      "Epoch: 12, Batch: 500, Loss: 2.7062\n",
      "Epoch: 12, Batch: 600, Loss: 2.5323\n",
      "Epoch: 12, Batch: 700, Loss: 2.8388\n",
      "Epoch: 12, Batch: 800, Loss: 2.5365\n",
      "Epoch: 12, Batch: 900, Loss: 2.2295\n",
      "Epoch: 12, Batch: 1000, Loss: 2.3624\n",
      "Epoch: 12, Batch: 1100, Loss: 2.4928\n",
      "Epoch: 12, Batch: 1200, Loss: 2.2498\n",
      "Epoch: 12, Batch: 1300, Loss: 2.9725\n",
      "Epoch: 12, Batch: 1400, Loss: 2.9956\n",
      "Epoch: 12, Batch: 1500, Loss: 2.9327\n",
      "Epoch: 12, Batch: 1600, Loss: 2.5061\n",
      "Epoch: 12, Batch: 1700, Loss: 2.7657\n",
      "Epoch: 12, Batch: 1800, Loss: 2.7246\n",
      "Epoch: 12, Batch: 1900, Loss: 2.4751\n",
      "Epoch: 12, Batch: 2000, Loss: 2.4094\n",
      "Epoch: 12, Batch: 2100, Loss: 2.5088\n",
      "Epoch: 12, Batch: 2200, Loss: 2.8382\n",
      "Epoch: 12, Batch: 2300, Loss: 2.6227\n",
      "Epoch: 12, Batch: 2400, Loss: 2.6250\n",
      "Epoch: 12, Batch: 2500, Loss: 2.6746\n",
      "Epoch: 12, Batch: 2600, Loss: 2.7136\n",
      "Epoch: 13, Batch: 100, Loss: 2.7292\n",
      "Epoch: 13, Batch: 200, Loss: 2.2577\n",
      "Epoch: 13, Batch: 300, Loss: 2.2055\n",
      "Epoch: 13, Batch: 400, Loss: 2.4177\n",
      "Epoch: 13, Batch: 500, Loss: 2.5844\n",
      "Epoch: 13, Batch: 600, Loss: 2.4495\n",
      "Epoch: 13, Batch: 700, Loss: 2.7225\n",
      "Epoch: 13, Batch: 800, Loss: 2.6130\n",
      "Epoch: 13, Batch: 900, Loss: 2.1969\n",
      "Epoch: 13, Batch: 1000, Loss: 2.6267\n",
      "Epoch: 13, Batch: 1100, Loss: 2.5086\n",
      "Epoch: 13, Batch: 1200, Loss: 2.2628\n",
      "Epoch: 13, Batch: 1300, Loss: 2.5544\n",
      "Epoch: 13, Batch: 1400, Loss: 2.5113\n",
      "Epoch: 13, Batch: 1500, Loss: 1.8731\n",
      "Epoch: 13, Batch: 1600, Loss: 2.6703\n",
      "Epoch: 13, Batch: 1700, Loss: 2.9808\n",
      "Epoch: 13, Batch: 1800, Loss: 2.6636\n",
      "Epoch: 13, Batch: 1900, Loss: 2.7712\n",
      "Epoch: 13, Batch: 2000, Loss: 2.7254\n",
      "Epoch: 13, Batch: 2100, Loss: 2.4464\n",
      "Epoch: 13, Batch: 2200, Loss: 2.7777\n",
      "Epoch: 13, Batch: 2300, Loss: 2.7233\n",
      "Epoch: 13, Batch: 2400, Loss: 2.1427\n",
      "Epoch: 13, Batch: 2500, Loss: 2.7414\n",
      "Epoch: 13, Batch: 2600, Loss: 2.6482\n",
      "Epoch: 14, Batch: 100, Loss: 2.3829\n",
      "Epoch: 14, Batch: 200, Loss: 1.9988\n",
      "Epoch: 14, Batch: 300, Loss: 2.2930\n",
      "Epoch: 14, Batch: 400, Loss: 2.0777\n",
      "Epoch: 14, Batch: 500, Loss: 1.8875\n",
      "Epoch: 14, Batch: 600, Loss: 2.4362\n",
      "Epoch: 14, Batch: 700, Loss: 2.0368\n",
      "Epoch: 14, Batch: 800, Loss: 2.1366\n",
      "Epoch: 14, Batch: 900, Loss: 2.4437\n",
      "Epoch: 14, Batch: 1000, Loss: 2.4074\n",
      "Epoch: 14, Batch: 1100, Loss: 2.4796\n",
      "Epoch: 14, Batch: 1200, Loss: 2.7726\n",
      "Epoch: 14, Batch: 1300, Loss: 2.0944\n",
      "Epoch: 14, Batch: 1400, Loss: 2.2604\n",
      "Epoch: 14, Batch: 1500, Loss: 2.1866\n",
      "Epoch: 14, Batch: 1600, Loss: 2.5032\n",
      "Epoch: 14, Batch: 1700, Loss: 2.1643\n",
      "Epoch: 14, Batch: 1800, Loss: 2.2772\n",
      "Epoch: 14, Batch: 1900, Loss: 2.1628\n",
      "Epoch: 14, Batch: 2000, Loss: 2.5991\n",
      "Epoch: 14, Batch: 2100, Loss: 2.8453\n",
      "Epoch: 14, Batch: 2200, Loss: 2.5377\n",
      "Epoch: 14, Batch: 2300, Loss: 2.6863\n",
      "Epoch: 14, Batch: 2400, Loss: 2.3262\n",
      "Epoch: 14, Batch: 2500, Loss: 2.3716\n",
      "Epoch: 14, Batch: 2600, Loss: 2.1894\n",
      "Epoch: 15, Batch: 100, Loss: 2.0545\n",
      "Epoch: 15, Batch: 200, Loss: 2.1153\n",
      "Epoch: 15, Batch: 300, Loss: 2.3031\n",
      "Epoch: 15, Batch: 400, Loss: 1.7797\n",
      "Epoch: 15, Batch: 500, Loss: 1.9117\n",
      "Epoch: 15, Batch: 600, Loss: 2.3565\n",
      "Epoch: 15, Batch: 700, Loss: 2.0271\n",
      "Epoch: 15, Batch: 800, Loss: 1.8247\n",
      "Epoch: 15, Batch: 900, Loss: 2.2876\n",
      "Epoch: 15, Batch: 1000, Loss: 2.1191\n",
      "Epoch: 15, Batch: 1100, Loss: 2.4327\n",
      "Epoch: 15, Batch: 1200, Loss: 2.0913\n",
      "Epoch: 15, Batch: 1300, Loss: 2.4403\n",
      "Epoch: 15, Batch: 1400, Loss: 2.9649\n",
      "Epoch: 15, Batch: 1500, Loss: 2.6369\n",
      "Epoch: 15, Batch: 1600, Loss: 2.3492\n",
      "Epoch: 15, Batch: 1700, Loss: 1.8133\n",
      "Epoch: 15, Batch: 1800, Loss: 2.5876\n",
      "Epoch: 15, Batch: 1900, Loss: 2.3331\n",
      "Epoch: 15, Batch: 2000, Loss: 1.9425\n",
      "Epoch: 15, Batch: 2100, Loss: 2.3727\n",
      "Epoch: 15, Batch: 2200, Loss: 2.0586\n",
      "Epoch: 15, Batch: 2300, Loss: 2.3451\n",
      "Epoch: 15, Batch: 2400, Loss: 2.5029\n",
      "Epoch: 15, Batch: 2500, Loss: 2.5111\n",
      "Epoch: 15, Batch: 2600, Loss: 1.9807\n",
      "Epoch: 16, Batch: 100, Loss: 2.0277\n",
      "Epoch: 16, Batch: 200, Loss: 2.1798\n",
      "Epoch: 16, Batch: 300, Loss: 1.7398\n",
      "Epoch: 16, Batch: 400, Loss: 1.6990\n",
      "Epoch: 16, Batch: 500, Loss: 1.9640\n",
      "Epoch: 16, Batch: 600, Loss: 2.1562\n",
      "Epoch: 16, Batch: 700, Loss: 1.7931\n",
      "Epoch: 16, Batch: 800, Loss: 2.2267\n",
      "Epoch: 16, Batch: 900, Loss: 2.2448\n",
      "Epoch: 16, Batch: 1000, Loss: 2.2440\n",
      "Epoch: 16, Batch: 1100, Loss: 2.0995\n",
      "Epoch: 16, Batch: 1200, Loss: 2.0456\n",
      "Epoch: 16, Batch: 1300, Loss: 2.1245\n",
      "Epoch: 16, Batch: 1400, Loss: 2.1501\n",
      "Epoch: 16, Batch: 1500, Loss: 2.1135\n",
      "Epoch: 16, Batch: 1600, Loss: 1.9156\n",
      "Epoch: 16, Batch: 1700, Loss: 1.9078\n",
      "Epoch: 16, Batch: 1800, Loss: 2.3016\n",
      "Epoch: 16, Batch: 1900, Loss: 2.0064\n",
      "Epoch: 16, Batch: 2000, Loss: 2.1107\n",
      "Epoch: 16, Batch: 2100, Loss: 1.9071\n",
      "Epoch: 16, Batch: 2200, Loss: 2.3379\n",
      "Epoch: 16, Batch: 2300, Loss: 2.6798\n",
      "Epoch: 16, Batch: 2400, Loss: 1.8105\n",
      "Epoch: 16, Batch: 2500, Loss: 1.9546\n",
      "Epoch: 16, Batch: 2600, Loss: 2.2888\n",
      "Epoch: 17, Batch: 100, Loss: 2.2249\n",
      "Epoch: 17, Batch: 200, Loss: 1.6970\n",
      "Epoch: 17, Batch: 300, Loss: 2.0360\n",
      "Epoch: 17, Batch: 400, Loss: 1.9921\n",
      "Epoch: 17, Batch: 500, Loss: 1.5973\n",
      "Epoch: 17, Batch: 600, Loss: 1.7162\n",
      "Epoch: 17, Batch: 700, Loss: 1.7709\n",
      "Epoch: 17, Batch: 800, Loss: 1.4121\n",
      "Epoch: 17, Batch: 900, Loss: 1.7475\n",
      "Epoch: 17, Batch: 1000, Loss: 2.0098\n",
      "Epoch: 17, Batch: 1100, Loss: 1.6734\n",
      "Epoch: 17, Batch: 1200, Loss: 2.1784\n",
      "Epoch: 17, Batch: 1300, Loss: 1.8023\n",
      "Epoch: 17, Batch: 1400, Loss: 2.0587\n",
      "Epoch: 17, Batch: 1500, Loss: 1.6959\n",
      "Epoch: 17, Batch: 1600, Loss: 1.6844\n",
      "Epoch: 17, Batch: 1700, Loss: 1.6962\n",
      "Epoch: 17, Batch: 1800, Loss: 2.0466\n",
      "Epoch: 17, Batch: 1900, Loss: 2.2651\n",
      "Epoch: 17, Batch: 2000, Loss: 1.7472\n",
      "Epoch: 17, Batch: 2100, Loss: 2.2034\n",
      "Epoch: 17, Batch: 2200, Loss: 1.8584\n",
      "Epoch: 17, Batch: 2300, Loss: 1.7685\n",
      "Epoch: 17, Batch: 2400, Loss: 1.8993\n",
      "Epoch: 17, Batch: 2500, Loss: 1.9653\n",
      "Epoch: 17, Batch: 2600, Loss: 1.9919\n",
      "Epoch: 18, Batch: 100, Loss: 1.4010\n",
      "Epoch: 18, Batch: 200, Loss: 1.4645\n",
      "Epoch: 18, Batch: 300, Loss: 2.0176\n",
      "Epoch: 18, Batch: 400, Loss: 1.7309\n",
      "Epoch: 18, Batch: 500, Loss: 1.8065\n",
      "Epoch: 18, Batch: 600, Loss: 1.5614\n",
      "Epoch: 18, Batch: 700, Loss: 1.5976\n",
      "Epoch: 18, Batch: 800, Loss: 1.7602\n",
      "Epoch: 18, Batch: 900, Loss: 2.0364\n",
      "Epoch: 18, Batch: 1000, Loss: 1.7393\n",
      "Epoch: 18, Batch: 1100, Loss: 1.7073\n",
      "Epoch: 18, Batch: 1200, Loss: 1.5106\n",
      "Epoch: 18, Batch: 1300, Loss: 1.5100\n",
      "Epoch: 18, Batch: 1400, Loss: 1.5179\n",
      "Epoch: 18, Batch: 1500, Loss: 1.7732\n",
      "Epoch: 18, Batch: 1600, Loss: 1.5769\n",
      "Epoch: 18, Batch: 1700, Loss: 1.5045\n",
      "Epoch: 18, Batch: 1800, Loss: 1.4106\n",
      "Epoch: 18, Batch: 1900, Loss: 1.9352\n",
      "Epoch: 18, Batch: 2000, Loss: 1.6433\n",
      "Epoch: 18, Batch: 2100, Loss: 1.9828\n",
      "Epoch: 18, Batch: 2200, Loss: 1.7301\n",
      "Epoch: 18, Batch: 2300, Loss: 1.7614\n",
      "Epoch: 18, Batch: 2400, Loss: 1.3622\n",
      "Epoch: 18, Batch: 2500, Loss: 1.9702\n",
      "Epoch: 18, Batch: 2600, Loss: 1.5873\n",
      "Epoch: 19, Batch: 100, Loss: 1.3140\n",
      "Epoch: 19, Batch: 200, Loss: 1.4839\n",
      "Epoch: 19, Batch: 300, Loss: 1.7107\n",
      "Epoch: 19, Batch: 400, Loss: 1.8609\n",
      "Epoch: 19, Batch: 500, Loss: 1.7028\n",
      "Epoch: 19, Batch: 600, Loss: 1.2714\n",
      "Epoch: 19, Batch: 700, Loss: 1.5273\n",
      "Epoch: 19, Batch: 800, Loss: 1.6747\n",
      "Epoch: 19, Batch: 900, Loss: 2.1206\n",
      "Epoch: 19, Batch: 1000, Loss: 1.9813\n",
      "Epoch: 19, Batch: 1100, Loss: 1.8020\n",
      "Epoch: 19, Batch: 1200, Loss: 1.8392\n",
      "Epoch: 19, Batch: 1300, Loss: 1.8382\n",
      "Epoch: 19, Batch: 1400, Loss: 1.7904\n",
      "Epoch: 19, Batch: 1500, Loss: 1.6221\n",
      "Epoch: 19, Batch: 1600, Loss: 1.8873\n",
      "Epoch: 19, Batch: 1700, Loss: 1.4243\n",
      "Epoch: 19, Batch: 1800, Loss: 1.8496\n",
      "Epoch: 19, Batch: 1900, Loss: 1.4149\n",
      "Epoch: 19, Batch: 2000, Loss: 1.4157\n",
      "Epoch: 19, Batch: 2100, Loss: 1.7805\n",
      "Epoch: 19, Batch: 2200, Loss: 1.4211\n",
      "Epoch: 19, Batch: 2300, Loss: 1.7804\n",
      "Epoch: 19, Batch: 2400, Loss: 1.8687\n",
      "Epoch: 19, Batch: 2500, Loss: 1.3902\n",
      "Epoch: 19, Batch: 2600, Loss: 1.8973\n",
      "Epoch: 20, Batch: 100, Loss: 1.9910\n",
      "Epoch: 20, Batch: 200, Loss: 1.7362\n",
      "Epoch: 20, Batch: 300, Loss: 1.2120\n",
      "Epoch: 20, Batch: 400, Loss: 1.8969\n",
      "Epoch: 20, Batch: 500, Loss: 1.4117\n",
      "Epoch: 20, Batch: 600, Loss: 1.1844\n",
      "Epoch: 20, Batch: 700, Loss: 1.0797\n",
      "Epoch: 20, Batch: 800, Loss: 1.5041\n",
      "Epoch: 20, Batch: 900, Loss: 1.8824\n",
      "Epoch: 20, Batch: 1000, Loss: 1.7803\n",
      "Epoch: 20, Batch: 1100, Loss: 1.4919\n",
      "Epoch: 20, Batch: 1200, Loss: 1.6465\n",
      "Epoch: 20, Batch: 1300, Loss: 1.4371\n",
      "Epoch: 20, Batch: 1400, Loss: 1.5499\n",
      "Epoch: 20, Batch: 1500, Loss: 1.8128\n",
      "Epoch: 20, Batch: 1600, Loss: 1.5857\n",
      "Epoch: 20, Batch: 1700, Loss: 1.5672\n",
      "Epoch: 20, Batch: 1800, Loss: 1.6630\n",
      "Epoch: 20, Batch: 1900, Loss: 1.6213\n",
      "Epoch: 20, Batch: 2000, Loss: 1.6347\n",
      "Epoch: 20, Batch: 2100, Loss: 1.7729\n",
      "Epoch: 20, Batch: 2200, Loss: 1.5647\n",
      "Epoch: 20, Batch: 2300, Loss: 1.7174\n",
      "Epoch: 20, Batch: 2400, Loss: 1.4617\n",
      "Epoch: 20, Batch: 2500, Loss: 1.8714\n",
      "Epoch: 20, Batch: 2600, Loss: 1.9297\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (seq, label) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(seq)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = seed_text.split()\n",
    "        token_list = token_list[-max_sequence_len:]\n",
    "        token_list = [word_2_index[word] for word in token_list]\n",
    "\n",
    "        token_list = torch.tensor(token_list, dtype=torch.long).unsqueeze(0)\n",
    "        output = model(token_list)\n",
    "        _, output = torch.max(output, dim=1)\n",
    "        output_word = index_to_word[output.item()]\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be that a break tis purple you the i hand i strike\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"to be or not to be that\", 10, model, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to be or not to be that a break tis purple you the i hand i strike\n"
     ]
    }
   ],
   "source": [
    "# saving the model\n",
    "torch.save(model.state_dict(), \"./models/ffn_lm.pth\")\n",
    "\n",
    "# loading the model\n",
    "model = FFn_LM(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "model.load_state_dict(torch.load(\"./models/ffn_lm.pth\"))\n",
    "\n",
    "print(generate_text(\"to be or not to be that\", 10, model, 8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
