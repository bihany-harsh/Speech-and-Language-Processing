{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feedforward neural network for Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../naive_bayes/movie_data.csv\", encoding=\"utf-8\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the text data\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # remove all non-word characters, convert to lowercase, append the emoticons,\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return text\n",
    "\n",
    "def tokenizer(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [w for w in text if w not in stop]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df[\"review\"].apply(preprocessor).apply(tokenizer).apply(remove_stopwords).apply(word_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [1974, teenager, martha, moxley, maggie, grace...\n",
       "1        [ok, really, like, kris, kristofferson, usual,...\n",
       "2        [spoiler, read, think, watching, movie, althou...\n",
       "3        [hi, people, seen, wonderful, movie, im, sure,...\n",
       "4        [recently, bought, dvd, forgetting, much, hate...\n",
       "                               ...                        \n",
       "49995    [ok, let, start, best, building, although, har...\n",
       "49996    [british, heritage, film, industry, control, n...\n",
       "49997    [even, know, begin, one, family, worst, line, ...\n",
       "49998    [richard, tyler, little, boy, scared, everythi...\n",
       "49999    [waited, long, watch, movie, also, like, bruce...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILDING OUR EMBEDDING MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=reviews, vector_size=128, window=5, min_count=1, workers=4, sg=1, negative=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56937517, 59792650)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.train(reviews, total_examples=word2vec_model.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv[\"movie\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aragorn', 0.7307092547416687),\n",
       " ('hobbit', 0.6770522594451904),\n",
       " ('bilbo', 0.673820972442627),\n",
       " ('saurmon', 0.6453695893287659),\n",
       " ('thun', 0.6447776556015015),\n",
       " ('orcs', 0.6405193209648132),\n",
       " ('almghandi', 0.6402015686035156),\n",
       " ('galadriel', 0.6353176832199097),\n",
       " ('balrog', 0.6341960430145264),\n",
       " ('gollum', 0.6246077418327332)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.most_similar(\"gandalf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\"./models\"):\n",
    "    os.makedirs(\"./models\")\n",
    "\n",
    "word2vec_model.save(\"./models/word2vec_model_IMDB.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aragorn', 0.7307092547416687),\n",
       " ('hobbit', 0.6770522594451904),\n",
       " ('bilbo', 0.673820972442627),\n",
       " ('saurmon', 0.6453695893287659),\n",
       " ('thun', 0.6447776556015015),\n",
       " ('orcs', 0.6405193209648132),\n",
       " ('almghandi', 0.6402015686035156),\n",
       " ('galadriel', 0.6353176832199097),\n",
       " ('balrog', 0.6341960430145264),\n",
       " ('gollum', 0.6246077418327332)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the model\n",
    "word2vec_model = Word2Vec.load(\"./models/word2vec_model_IMDB.model\")\n",
    "word2vec_model.wv.most_similar(\"gandalf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "1 ['guess', 'one', 'sided', 'relationship', 'sort', 'able', 'identify', 'lead', 'character', 'minako', 'yuko', 'tanaka', '50', 'year', 'old', 'woman', 'still', 'pink', 'good', 'health', 'demonstrated', 'daily', 'grinding', 'routine', 'waking', 'extremely', 'early', 'morning', 'prepare', 'milk', 'delivery', 'work', 'lug', 'bottle', 'megmilk', 'bag', 'route', 'around', 'town', 'like', 'clockwork', 'exchange', 'empty', 'bottle', 'full', 'one', 'collect', 'payment', 'issue', 'receipt', 'always', 'one', 'delivery', 'stop', 'right', 'top', 'needing', 'scale', 'long', 'flight', 'stair', 'order', 'achieve', 'customer', 'satisfaction', 'peculiar', 'enough', 'stop', 'happened', 'stop', 'delivering', 'man', 'love', 'almost', 'teenage', 'adult', 'life', 'product', 'appreciated', 'poured', 'sink', 'gone', 'school', 'see', 'talking', 'daily', 'life', 'always', 'seem', 'close', 'physically', 'yet', 'far', 'away', 'eye', 'contact', 'save', 'cursory', 'glance', 'chance', 'little', 'acknowledgement', 'existence', 'learn', 'share', 'past', 'probably', 'destroyed', 'notion', 'together', 'clear', 'attraction', 'two', 'hampered', 'developing', 'earlier', 'generation', 'thought', 'minako', 'interesting', 'woman', 'one', 'kept', 'feeling', 'suppressed', 'long', 'one', 'wonder', 'kind', 'damage', 'would', 'read', 'original', 'japanese', 'title', 'mean', 'time', 'day', 'read', 'book', 'accurate', 'felt', 'movie', 'wonderful', 'finale', 'shot', 'well', 'stocked', 'bookcase', 'likely', 'alluding', 'fact', 'alone', 'probably', 'fallen', 'back', 'crutch', 'sort', 'deal', 'pain', 'alone', 'back', 'lifestyle', 'already', 'accustomed', '50', 'year', 'besides', 'immersing', 'two', 'job', 'book', 'serve', 'form', 'escapism', 'occasionally', 'pen', 'little', 'sweet', 'nothing', 'song', 'dedication', 'show', 'radio', 'yuko', 'tanaka', 'commendable', 'job', 'emotionally', 'strong', 'woman', 'resigned', 'fate', 'decision', 'love', 'none', 'object', 'affection', 'takanashi', 'ittoku', 'kishibe', 'interesting', 'character', 'facet', 'staying', 'true', 'marriage', 'vow', 'spends', 'significant', 'amount', 'screen', 'time', 'looking', 'sickly', 'bedridden', 'wife', 'played', 'akiko', 'nishina', 'juggling', 'job', 'social', 'welfare', 'child', 'affair', 'department', 'city', 'hall', 'felt', 'childless', 'couple', 'job', 'provided', 'mean', 'care', 'people', 'child', 'troubled', 'one', 'neglected', 'left', 'fend', 'rare', 'moment', 'rage', 'see', 'angrily', 'chides', 'wayward', 'parent', 'appreciate', 'waste', 'child', 'life', 'away', 'story', 'kenji', 'aoki', 'provides', 'little', 'quirk', 'make', 'character', 'appeal', 'successfully', 'attempted', 'provide', 'lot', 'glimpse', 'dimension', 'well', 'takanashi', 'hopeless', 'haiku', 'poet', 'despite', 'member', 'haiku', 'club', 'supporting', 'character', 'aged', 'minagawa', 'couple', 'masao', 'koichi', 'ueda', 'lent', 'comical', 'though', 'sad', 'moment', 'slowly', 'turned', 'senile', 'wife', 'toshiko', 'misako', 'watanabe', 'narrates', 'brings', 'u', 'love', 'story', 'single', 'woman', '50', 'even', 'akiko', 'nishina', 'performance', 'bedridden', 'wife', 'nothing', 'short', 'arresting', 'character', 'enlightened', 'state', 'knowing', 'husband', 'past', 'making', 'unselfish', 'painful', 'decision', 'sickly', 'state', 'expect', 'typical', 'japanese', 'romantic', 'movie', 'sans', 'young', 'nubile', 'lead', 'star', 'crossed', 'lover', 'element', 'place', 'romantic', 'set', 'ups', 'love', 'song', 'quintessential', 'restrained', 'affectionate', 'behaviour', 'thought', 'story', 'danger', 'going', 'beaten', 'track', 'unrequited', 'love', 'get', 'consummated', 'director', 'akira', 'ogata', 'managed', 'steer', 'clear', 'usual', 'melodramatic', 'moment', 'story', 'though', 'story', 'call', 'obvious', 'plot', 'development', 'final', 'act', 'predict', 'especially', 'already', 'way', 'past', 'romance', 'movie', '101', 'average', 'lovey', 'dovey', 'story', 'thought', 'milkwoman', 'told', 'strong', 'story', 'unrequited', 'love', 'central', 'theme', 'frankly', 'recommended', 'romance', 'movie', 'though', 'told', 'measured', 'pace', 'mood', 'bittersweet', 'loving', 'reminiscence', 'seeking', 'live', 'without', 'regret']\n"
     ]
    }
   ],
   "source": [
    "labels = df[\"sentiment\"].values\n",
    "print(len(labels))\n",
    "print(labels[34], reviews[34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df[\"sentiment\"].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ffn_classification](./images/ffn_class.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING THE FEATURE VECTORS (sum of embeddings in a text sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the feature vectors which would be sum of all the word vectors in a review\n",
    "\n",
    "def build_feature_vectors(text, model, embed_size):\n",
    "    feature_vector = np.zeros((embed_size,), dtype=\"float32\")\n",
    "    num_words = 0\n",
    "    for word in text:\n",
    "        if word in model.wv:\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "            num_words += 1\n",
    "    if num_words:\n",
    "        feature_vector = np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function to all the reviews\n",
    "feature_vectors = [build_feature_vectors(text, word2vec_model, 128) for text in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 128)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors = np.array(feature_vectors)\n",
    "feature_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 128), (10000, 128), (40000,), (10000,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUILD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 300)               38700     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                19264     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,077\n",
      "Trainable params: 60,077\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(300, input_dim=128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-11 17:57:01.023991: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - 10s 8ms/step - loss: 0.3463 - accuracy: 0.8516 - val_loss: 0.2940 - val_accuracy: 0.8813\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 0.3050 - accuracy: 0.8724 - val_loss: 0.2842 - val_accuracy: 0.8842\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 0.2994 - accuracy: 0.8761 - val_loss: 0.3063 - val_accuracy: 0.8742\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 0.2931 - accuracy: 0.8776 - val_loss: 0.2836 - val_accuracy: 0.8858\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 0.2909 - accuracy: 0.8782 - val_loss: 0.2766 - val_accuracy: 0.8878\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 0.2872 - accuracy: 0.8806 - val_loss: 0.2942 - val_accuracy: 0.8771\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 0.2831 - accuracy: 0.8831 - val_loss: 0.2821 - val_accuracy: 0.8879\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 0.2809 - accuracy: 0.8836 - val_loss: 0.2953 - val_accuracy: 0.8798\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 0.2796 - accuracy: 0.8859 - val_loss: 0.2773 - val_accuracy: 0.8881\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 9s 8ms/step - loss: 0.2763 - accuracy: 0.8860 - val_loss: 0.2750 - val_accuracy: 0.8867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2cb469270>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 5ms/step - loss: 0.2750 - accuracy: 0.8867\n",
      "Test accuracy:  0.8866999745368958\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy: \", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "if not os.path.exists(\"./models\"):\n",
    "    os.makedirs(\"./models\")\n",
    "\n",
    "model.save(\"./models/ffn_classification_IMDB.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 283ms/step\n",
      "Positive review\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"./models/ffn_classification_IMDB.h5\")\n",
    "\n",
    "# such an awesome movie\n",
    "review = \"such an awesome movie\"\n",
    "review = preprocessor(review)\n",
    "review = tokenizer(review)\n",
    "review = remove_stopwords(review)\n",
    "review = word_lemmatizer(review)\n",
    "\n",
    "review = build_feature_vectors(review, word2vec_model, 128)\n",
    "review = np.array(review).reshape(1, -1)\n",
    "\n",
    "pred = model.predict(review)\n",
    "\n",
    "if pred > 0.5:\n",
    "    print(\"Positive review\")\n",
    "else:\n",
    "    print(\"Negative review\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
