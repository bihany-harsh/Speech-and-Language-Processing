{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneser_ney_interpolation import KneserNeyInterpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "tokens = ['this', 'is', 'a', 'sample', 'text', 'for', 'testing', 'the', 'language', 'model']\n",
    "\n",
    "kni = KneserNeyInterpolated(order=3, discount=0.75)\n",
    "kni.train(tokens=tokens)\n",
    "\n",
    "ngram = ('is', 'a', 'sample')\n",
    "print(kni.logscore(ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(kni.prob(ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sample text for testing the language model sample sample sample sample sample sample sample sample sample sample\n"
     ]
    }
   ],
   "source": [
    "generated_tokens = kni.generate(context=[\"this\", \"is\"], max_length=20)\n",
    "print(' '.join(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "with io.open(os.path.join(\"../data/\", \"language-never-random.txt\"), encoding='utf8') as fin:\n",
    "    text = fin.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = text.split()\n",
    "# lower case\n",
    "tokens = [token.lower() for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['language',\n",
       " 'is',\n",
       " 'never,',\n",
       " 'ever,',\n",
       " 'ever,',\n",
       " 'random',\n",
       " 'adam',\n",
       " 'kilgarriff',\n",
       " 'abstract',\n",
       " 'language',\n",
       " 'users',\n",
       " 'never',\n",
       " 'choose',\n",
       " 'words',\n",
       " 'randomly,',\n",
       " 'and',\n",
       " 'language',\n",
       " 'is',\n",
       " 'essentially',\n",
       " 'non-random.',\n",
       " 'statistical',\n",
       " 'hypothesis',\n",
       " 'testing',\n",
       " 'uses',\n",
       " 'a',\n",
       " 'null',\n",
       " 'hypothesis,',\n",
       " 'which',\n",
       " 'posits',\n",
       " 'randomness.',\n",
       " 'hence,',\n",
       " 'when',\n",
       " 'we',\n",
       " 'look',\n",
       " 'at',\n",
       " 'linguistic',\n",
       " 'phenomena',\n",
       " 'in',\n",
       " 'cor-',\n",
       " 'pora,',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'will',\n",
       " 'never',\n",
       " 'be',\n",
       " 'true.',\n",
       " 'moreover,',\n",
       " 'where',\n",
       " 'there',\n",
       " 'is',\n",
       " 'enough',\n",
       " 'data,',\n",
       " 'we',\n",
       " 'shall',\n",
       " '(almost)',\n",
       " 'always',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'establish',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'true.',\n",
       " 'in',\n",
       " 'corpus',\n",
       " 'studies,',\n",
       " 'we',\n",
       " 'frequently',\n",
       " 'do',\n",
       " 'have',\n",
       " 'enough',\n",
       " 'data,',\n",
       " 'so',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'a',\n",
       " 'rela-',\n",
       " 'tion',\n",
       " 'between',\n",
       " 'two',\n",
       " 'phenomena',\n",
       " 'is',\n",
       " 'demonstrably',\n",
       " 'non-random,',\n",
       " 'does',\n",
       " 'not',\n",
       " 'sup-',\n",
       " 'port',\n",
       " 'the',\n",
       " 'inference',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'arbitrary.',\n",
       " 'we',\n",
       " 'present',\n",
       " 'experimental',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'how',\n",
       " 'arbitrary',\n",
       " 'associations',\n",
       " 'between',\n",
       " 'word',\n",
       " 'frequencies',\n",
       " 'and',\n",
       " 'corpora',\n",
       " 'are',\n",
       " 'systematically',\n",
       " 'non-random.',\n",
       " 'we',\n",
       " 'review',\n",
       " 'literature',\n",
       " 'in',\n",
       " 'which',\n",
       " 'hypothesis',\n",
       " 'test-',\n",
       " 'ing',\n",
       " 'has',\n",
       " 'been',\n",
       " 'used,',\n",
       " 'and',\n",
       " 'show',\n",
       " 'how',\n",
       " 'it',\n",
       " 'has',\n",
       " 'often',\n",
       " 'led',\n",
       " 'to',\n",
       " 'unhelpful',\n",
       " 'or',\n",
       " 'mislead-',\n",
       " 'ing',\n",
       " 'results.',\n",
       " 'keywords:',\n",
       " '쎲쎲쎲',\n",
       " '1.',\n",
       " 'introduction',\n",
       " 'any',\n",
       " 'two',\n",
       " 'phenomena',\n",
       " 'might',\n",
       " 'or',\n",
       " 'might',\n",
       " 'not',\n",
       " 'be',\n",
       " 'related.',\n",
       " 'the',\n",
       " 'range',\n",
       " 'of',\n",
       " 'pos-',\n",
       " 'sibilities',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'association',\n",
       " 'is',\n",
       " 'random,',\n",
       " 'arbitrary,',\n",
       " 'motivated',\n",
       " 'or',\n",
       " 'pre-',\n",
       " 'dictable',\n",
       " '(r,',\n",
       " 'a,',\n",
       " 'm,',\n",
       " 'p).',\n",
       " 'the',\n",
       " 'bulk',\n",
       " 'of',\n",
       " 'linguistic',\n",
       " 'questions',\n",
       " 'concern',\n",
       " 'the',\n",
       " 'dis-',\n",
       " 'tinction',\n",
       " 'between',\n",
       " 'a',\n",
       " 'and',\n",
       " 'm.',\n",
       " 'a',\n",
       " 'linguistic',\n",
       " 'account',\n",
       " 'of',\n",
       " 'a',\n",
       " 'phenomenon',\n",
       " 'gen-',\n",
       " 'erally',\n",
       " 'gives',\n",
       " 'us',\n",
       " 'reason',\n",
       " 'to',\n",
       " 'view',\n",
       " 'the',\n",
       " 'relation',\n",
       " 'between,',\n",
       " 'for',\n",
       " 'example,',\n",
       " 'a',\n",
       " 'verb’s',\n",
       " 'syntax',\n",
       " 'and',\n",
       " 'its',\n",
       " 'semantics,',\n",
       " 'as',\n",
       " 'motivated',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'arbitrary.',\n",
       " 'however,',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'in',\n",
       " 'general',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'model',\n",
       " 'the',\n",
       " 'a-m',\n",
       " 'distinction',\n",
       " 'mathematically.',\n",
       " 'the',\n",
       " 'distinction',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'modeled',\n",
       " 'mathematically',\n",
       " 'is',\n",
       " 'between',\n",
       " 'r',\n",
       " 'and',\n",
       " 'not-r,',\n",
       " 'that',\n",
       " 'is,',\n",
       " 'between',\n",
       " 'random,',\n",
       " 'or',\n",
       " 'uncorrelated,',\n",
       " 'pairs',\n",
       " 'and',\n",
       " 'pairs',\n",
       " 'where',\n",
       " 'there',\n",
       " 'is',\n",
       " 'some',\n",
       " 'correlation,',\n",
       " 'be',\n",
       " 'it',\n",
       " 'arbitrary,',\n",
       " 'motivated',\n",
       " 'or',\n",
       " 'predictable.1',\n",
       " 'the',\n",
       " 'mechanism',\n",
       " 'here',\n",
       " 'is',\n",
       " 'hypothesis-testing.',\n",
       " 'a',\n",
       " 'null',\n",
       " 'hypothesis,',\n",
       " 'h0',\n",
       " 'is',\n",
       " 'con-',\n",
       " 'structed',\n",
       " 'to',\n",
       " 'model',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'in',\n",
       " 'which',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'correlation',\n",
       " 'between',\n",
       " 'corpus',\n",
       " 'linguistics',\n",
       " 'and',\n",
       " 'linguistic',\n",
       " 'theory',\n",
       " '1⫺2',\n",
       " '(2005),',\n",
       " '263⫺275',\n",
       " '1613-7027/05/0001⫺0263',\n",
       " '쑕',\n",
       " 'walter',\n",
       " 'de',\n",
       " 'gruyter',\n",
       " '264',\n",
       " 'a.',\n",
       " 'kilgarriff',\n",
       " 'the',\n",
       " 'two',\n",
       " 'phenomena.',\n",
       " 'as',\n",
       " 'the',\n",
       " 'mathematics',\n",
       " 'of',\n",
       " 'the',\n",
       " 'random',\n",
       " 'is',\n",
       " 'well',\n",
       " 'under-',\n",
       " 'stood,',\n",
       " 'we',\n",
       " 'can',\n",
       " 'compute',\n",
       " 'the',\n",
       " 'likelihood',\n",
       " 'of',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'given',\n",
       " 'the',\n",
       " 'data.',\n",
       " 'if',\n",
       " 'the',\n",
       " 'likelihood',\n",
       " 'is',\n",
       " 'low,',\n",
       " 'we',\n",
       " 'reject',\n",
       " 'h0.',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'for',\n",
       " 'empirical',\n",
       " 'linguistics',\n",
       " 'is',\n",
       " 'that',\n",
       " 'language',\n",
       " 'is',\n",
       " 'not',\n",
       " 'random,',\n",
       " 'so',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'is',\n",
       " 'never',\n",
       " 'true.',\n",
       " 'language',\n",
       " 'is',\n",
       " 'not',\n",
       " 'random',\n",
       " 'because',\n",
       " 'we',\n",
       " 'speak',\n",
       " 'or',\n",
       " 'write',\n",
       " 'with',\n",
       " 'purposes.',\n",
       " 'we',\n",
       " 'do',\n",
       " 'not,',\n",
       " 'indeed,',\n",
       " 'without',\n",
       " 'computational',\n",
       " 'help',\n",
       " 'are',\n",
       " 'not',\n",
       " 'capable',\n",
       " 'of,',\n",
       " 'producing',\n",
       " 'words',\n",
       " 'or',\n",
       " 'sounds',\n",
       " 'or',\n",
       " 'sentences',\n",
       " 'or',\n",
       " 'documents',\n",
       " 'randomly.',\n",
       " 'we',\n",
       " 'do',\n",
       " 'not',\n",
       " 'always',\n",
       " 'have',\n",
       " 'enough',\n",
       " 'data',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis,',\n",
       " 'but',\n",
       " 'that',\n",
       " 'is',\n",
       " 'a',\n",
       " 'distinct',\n",
       " 'issue:',\n",
       " 'wherever',\n",
       " 'there',\n",
       " 'is',\n",
       " 'enough',\n",
       " 'data,',\n",
       " 'it',\n",
       " 'is',\n",
       " 'rejected.',\n",
       " 'using',\n",
       " 'language',\n",
       " 'corpora,',\n",
       " 'we',\n",
       " 'are',\n",
       " 'frequently',\n",
       " 'in',\n",
       " 'the',\n",
       " 'fortunate',\n",
       " 'position',\n",
       " 'of',\n",
       " 'having',\n",
       " 'very',\n",
       " 'large',\n",
       " 'quantities',\n",
       " 'of',\n",
       " 'data',\n",
       " 'at',\n",
       " 'our',\n",
       " 'disposal.',\n",
       " 'then,',\n",
       " 'even',\n",
       " 'where',\n",
       " 'pairs',\n",
       " 'of',\n",
       " 'corpora',\n",
       " 'are',\n",
       " 'set',\n",
       " 'up',\n",
       " 'to',\n",
       " 'be',\n",
       " 'linguistically',\n",
       " 'identical,',\n",
       " 'the',\n",
       " 'null',\n",
       " 'hypothesis',\n",
       " 'is',\n",
       " 'resoundingly',\n",
       " 'defeated.',\n",
       " 'in',\n",
       " 'section',\n",
       " '4,',\n",
       " 'we',\n",
       " 'present',\n",
       " 'an',\n",
       " 'experiment',\n",
       " 'demonstrating',\n",
       " 'this',\n",
       " 'counterintuitive',\n",
       " 'effect.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'papers',\n",
       " 'in',\n",
       " 'the',\n",
       " 'empirical',\n",
       " 'linguistics',\n",
       " 'literature',\n",
       " 'where',\n",
       " 'researchers',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'testing',\n",
       " 'whether',\n",
       " 'an',\n",
       " 'association',\n",
       " 'was',\n",
       " 'lin-',\n",
       " 'guistically',\n",
       " 'salient,',\n",
       " 'or',\n",
       " 'used',\n",
       " 'the',\n",
       " 'confidence',\n",
       " 'with',\n",
       " 'which',\n",
       " 'h0',\n",
       " 'could',\n",
       " 'be',\n",
       " 're-',\n",
       " 'jected',\n",
       " 'as',\n",
       " 'a',\n",
       " 'measure',\n",
       " 'of',\n",
       " 'salience,',\n",
       " 'whereas',\n",
       " 'in',\n",
       " 'fact',\n",
       " 'they',\n",
       " 'were',\n",
       " 'merely',\n",
       " 'testing',\n",
       " 'whether',\n",
       " 'they',\n",
       " 'had',\n",
       " 'enough',\n",
       " 'data',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'h0',\n",
       " 'with',\n",
       " 'confidence.',\n",
       " 'some',\n",
       " 'such',\n",
       " 'cases',\n",
       " 'are',\n",
       " 'reviewed',\n",
       " 'in',\n",
       " 'section',\n",
       " '5.',\n",
       " 'hypothesis',\n",
       " 'testing',\n",
       " 'has',\n",
       " 'been',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'in',\n",
       " 'the',\n",
       " 'acquisition',\n",
       " 'of',\n",
       " 'subcategorization',\n",
       " 'frames',\n",
       " 'from',\n",
       " 'corpora',\n",
       " 'and',\n",
       " 'this',\n",
       " 'literature',\n",
       " 'is',\n",
       " 'considered',\n",
       " 'in',\n",
       " 'some',\n",
       " 'detail.',\n",
       " 'alternatives',\n",
       " 'to',\n",
       " 'inappropriate',\n",
       " 'hy-',\n",
       " 'pothesis-testing',\n",
       " 'are',\n",
       " 'presented.',\n",
       " 'before',\n",
       " 'proceeding,',\n",
       " 'may',\n",
       " 'i',\n",
       " 'clarify',\n",
       " 'that',\n",
       " 'this',\n",
       " 'paper',\n",
       " 'is',\n",
       " 'in',\n",
       " 'no',\n",
       " 'way',\n",
       " 'critical',\n",
       " 'of',\n",
       " 'using',\n",
       " 'probability',\n",
       " 'models,',\n",
       " 'all',\n",
       " 'of',\n",
       " 'which',\n",
       " 'are',\n",
       " 'based',\n",
       " 'on',\n",
       " 'assumptions',\n",
       " 'of',\n",
       " 'randomness,',\n",
       " 'in',\n",
       " 'empirical',\n",
       " 'linguistics',\n",
       " 'in',\n",
       " 'general.',\n",
       " 'probability',\n",
       " 'models',\n",
       " 'have',\n",
       " 'been',\n",
       " 'responsible',\n",
       " 'for',\n",
       " 'a',\n",
       " 'large',\n",
       " 'share',\n",
       " 'of',\n",
       " 'progress',\n",
       " 'in',\n",
       " 'the',\n",
       " 'field',\n",
       " 'in',\n",
       " 'the',\n",
       " 'last',\n",
       " 'decade',\n",
       " 'and',\n",
       " 'a',\n",
       " 'half.',\n",
       " 'the',\n",
       " 'randomness',\n",
       " 'assumptions',\n",
       " 'are',\n",
       " 'always',\n",
       " 'untrue,',\n",
       " 'but',\n",
       " 'that',\n",
       " 'does',\n",
       " 'not',\n",
       " 'preclude',\n",
       " 'them',\n",
       " 'from',\n",
       " 'frequently',\n",
       " 'being',\n",
       " 'useful.',\n",
       " 'making',\n",
       " 'false',\n",
       " 'assumptions',\n",
       " 'is',\n",
       " 'often',\n",
       " 'an',\n",
       " 'ingenious',\n",
       " 'way',\n",
       " 'to',\n",
       " 'proceed;',\n",
       " 'the',\n",
       " 'problem',\n",
       " 'arises',\n",
       " 'where',\n",
       " 'the',\n",
       " 'literal',\n",
       " 'falsity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'assumption',\n",
       " 'is',\n",
       " 'overlooked,',\n",
       " 'and',\n",
       " 'inappropri-',\n",
       " 'ate',\n",
       " 'inferences',\n",
       " 'are',\n",
       " 'drawn.',\n",
       " '2.',\n",
       " 'the',\n",
       " 'arbitrary',\n",
       " 'and',\n",
       " 'the',\n",
       " 'random',\n",
       " 'in',\n",
       " 'common',\n",
       " 'parlance,',\n",
       " 'random',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'are',\n",
       " 'synonyms,',\n",
       " 'with',\n",
       " 'diction-',\n",
       " 'aries',\n",
       " 'giving',\n",
       " 'near-identical',\n",
       " 'definitions:',\n",
       " 'ldoce',\n",
       " '(1995)',\n",
       " 'defines',\n",
       " 'random',\n",
       " 'as',\n",
       " 'happening',\n",
       " 'or',\n",
       " 'chosen',\n",
       " 'without',\n",
       " 'any',\n",
       " 'definite',\n",
       " 'plan,',\n",
       " 'or',\n",
       " 'pattern',\n",
       " 'and',\n",
       " 'arbitrary',\n",
       " 'as',\n",
       " '1',\n",
       " 'decided',\n",
       " 'or',\n",
       " 'arranged',\n",
       " 'without',\n",
       " 'any',\n",
       " 'reason',\n",
       " 'or',\n",
       " 'plan,',\n",
       " 'often',\n",
       " 'unfairly',\n",
       " '…',\n",
       " '2',\n",
       " 'happening',\n",
       " 'or',\n",
       " 'decided',\n",
       " 'by',\n",
       " 'chance',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'a',\n",
       " 'plan',\n",
       " 'language',\n",
       " 'is',\n",
       " 'never,',\n",
       " 'ever,',\n",
       " 'ever,',\n",
       " 'random',\n",
       " '265',\n",
       " 'superficially,',\n",
       " 'randomness,',\n",
       " 'as',\n",
       " 'defined',\n",
       " 'here,',\n",
       " 'is',\n",
       " 'what',\n",
       " 'the',\n",
       " 'technical',\n",
       " 'sense',\n",
       " 'of',\n",
       " 'random',\n",
       " 'captures',\n",
       " 'and',\n",
       " 'makes',\n",
       " 'explicit.',\n",
       " 'the',\n",
       " 'technical',\n",
       " 'sense',\n",
       " 'is',\n",
       " 'defined',\n",
       " 'in',\n",
       " 'terms',\n",
       " 'of',\n",
       " 'statistical',\n",
       " 'independence.',\n",
       " 'first,',\n",
       " 'we',\n",
       " 'formalize',\n",
       " 'the',\n",
       " 'framework:',\n",
       " 'for',\n",
       " 'a',\n",
       " 'population',\n",
       " 'of',\n",
       " 'events,',\n",
       " 'the',\n",
       " 'first',\n",
       " 'phenomenon',\n",
       " 'holds',\n",
       " 'where',\n",
       " 'x',\n",
       " 'is',\n",
       " 'true',\n",
       " 'of',\n",
       " 'the',\n",
       " 'event,',\n",
       " 'the',\n",
       " 'second',\n",
       " 'holds',\n",
       " 'where',\n",
       " 'y',\n",
       " 'is',\n",
       " 'true',\n",
       " 'of',\n",
       " 'the',\n",
       " 'event.',\n",
       " 'now,',\n",
       " 'the',\n",
       " 'relation',\n",
       " 'between',\n",
       " 'the',\n",
       " 'phenomena',\n",
       " 'is',\n",
       " 'random',\n",
       " 'iff',\n",
       " 'the',\n",
       " 'prob-',\n",
       " 'ability',\n",
       " 'of',\n",
       " 'x,',\n",
       " 'for',\n",
       " 'that',\n",
       " 'subset',\n",
       " 'of',\n",
       " 'events',\n",
       " 'where',\n",
       " 'y',\n",
       " 'does',\n",
       " 'hold,',\n",
       " 'is',\n",
       " 'identical',\n",
       " 'to',\n",
       " 'its',\n",
       " 'probability',\n",
       " 'for',\n",
       " 'the',\n",
       " 'subset',\n",
       " 'where',\n",
       " 'y',\n",
       " 'does',\n",
       " 'not',\n",
       " 'hold,',\n",
       " 'that',\n",
       " 'is',\n",
       " 'p',\n",
       " '(x|y)',\n",
       " '⫽',\n",
       " 'p',\n",
       " '(x|ÿ',\n",
       " 'y)',\n",
       " 'the',\n",
       " 'relation',\n",
       " 'is',\n",
       " 'symmetric:',\n",
       " 'p',\n",
       " '(x|y)',\n",
       " '⫽',\n",
       " 'p',\n",
       " '(x|ÿ',\n",
       " 'y)',\n",
       " 'entails',\n",
       " 'p',\n",
       " '(y|x)',\n",
       " '⫽',\n",
       " 'p',\n",
       " '(y|ÿ',\n",
       " 'x).',\n",
       " 'hereafter',\n",
       " 'i',\n",
       " 'use',\n",
       " '‘random’',\n",
       " 'for',\n",
       " 'the',\n",
       " 'technical',\n",
       " 'meaning',\n",
       " 'and',\n",
       " '‘arbi-',\n",
       " 'trary’',\n",
       " 'for',\n",
       " 'the',\n",
       " 'non-technical',\n",
       " 'one.',\n",
       " 'arbitrary',\n",
       " 'events',\n",
       " 'are',\n",
       " 'very',\n",
       " 'rarely',\n",
       " 'random,',\n",
       " 'and',\n",
       " 'random',\n",
       " 'events',\n",
       " 'are',\n",
       " 'very',\n",
       " 'rarely',\n",
       " 'arbitrary.',\n",
       " 'it',\n",
       " 'takes',\n",
       " 'considerable',\n",
       " 'ingenuity',\n",
       " 'and',\n",
       " 'sophisticated',\n",
       " 'mathe-',\n",
       " 'matics',\n",
       " 'to',\n",
       " 'produce',\n",
       " 'a',\n",
       " 'pseudo-random',\n",
       " 'sequence',\n",
       " 'algorithmically,',\n",
       " 'and',\n",
       " 'true',\n",
       " 'randomness',\n",
       " 'is',\n",
       " 'not',\n",
       " 'possible',\n",
       " 'at',\n",
       " 'all.',\n",
       " 'events',\n",
       " 'happening',\n",
       " '“without',\n",
       " 'any',\n",
       " 'defi-',\n",
       " 'nite',\n",
       " 'plan,',\n",
       " 'aim',\n",
       " 'or',\n",
       " 'pattern”',\n",
       " 'are,',\n",
       " 'by',\n",
       " 'definition,',\n",
       " 'arbitrary,',\n",
       " 'but',\n",
       " 'are',\n",
       " 'vanish-',\n",
       " 'ingly',\n",
       " 'unlikely',\n",
       " 'to',\n",
       " 'be',\n",
       " 'random.',\n",
       " 'outside',\n",
       " 'the',\n",
       " 'sub-atomic',\n",
       " 'realm,',\n",
       " 'natural',\n",
       " 'events',\n",
       " 'are',\n",
       " 'very',\n",
       " 'rarely',\n",
       " 'random.',\n",
       " 'consider,',\n",
       " 'for',\n",
       " 'example,',\n",
       " 'cat',\n",
       " 'food',\n",
       " 'purchases',\n",
       " 'and',\n",
       " 'shoe-polish',\n",
       " 'purchases',\n",
       " 'within',\n",
       " 'the',\n",
       " 'space',\n",
       " 'of',\n",
       " 'all',\n",
       " 'uk',\n",
       " 'supermarket-shopping',\n",
       " 'events:',\n",
       " 'does',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'cat',\n",
       " 'food',\n",
       " 'was',\n",
       " 'bought',\n",
       " 'predict',\n",
       " '(positively',\n",
       " 'or',\n",
       " 'negatively)',\n",
       " 'whether',\n",
       " 'shoe',\n",
       " 'polish',\n",
       " 'was',\n",
       " 'bought',\n",
       " 'in',\n",
       " 'the',\n",
       " 'same',\n",
       " 'shopping',\n",
       " 'trip?',\n",
       " 'there',\n",
       " 'is',\n",
       " 'no',\n",
       " 'obvious',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'it',\n",
       " 'should,',\n",
       " 'and',\n",
       " 'we',\n",
       " 'can',\n",
       " 'happily',\n",
       " 'declare',\n",
       " 'the',\n",
       " 'relation',\n",
       " 'arbitrary.',\n",
       " 'but',\n",
       " 'perhaps',\n",
       " 'either',\n",
       " 'cat',\n",
       " 'food',\n",
       " 'or',\n",
       " 'shoe-polish',\n",
       " 'are',\n",
       " 'more',\n",
       " '(or',\n",
       " 'less)',\n",
       " 'often',\n",
       " 'bought',\n",
       " 'in',\n",
       " 'hot',\n",
       " '(or',\n",
       " 'cold)',\n",
       " 'weather,',\n",
       " 'or',\n",
       " 'on',\n",
       " 'saturday',\n",
       " 'nights,',\n",
       " 'or',\n",
       " 'sunday',\n",
       " 'mornings,',\n",
       " 'or',\n",
       " 'monday',\n",
       " 'lunchtimes,',\n",
       " 'or',\n",
       " 'by',\n",
       " 'richer',\n",
       " '(or',\n",
       " 'poorer)',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\testing.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Speech-and-Language-Processing/n_gram_models/kneser-ney-interpolation/testing.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m kni \u001b[39m=\u001b[39m KneserNeyInterpolated(order\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, discount\u001b[39m=\u001b[39m\u001b[39m0.75\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Speech-and-Language-Processing/n_gram_models/kneser-ney-interpolation/testing.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m kni\u001b[39m.\u001b[39mtrain(tokens\u001b[39m=\u001b[39mtokens)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Speech-and-Language-Processing/n_gram_models/kneser-ney-interpolation/testing.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m generated_tokens \u001b[39m=\u001b[39m kni\u001b[39m.\u001b[39;49mgenerate(context\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39madam\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mkilgarriff\u001b[39;49m\u001b[39m\"\u001b[39;49m], max_length\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Speech-and-Language-Processing/n_gram_models/kneser-ney-interpolation/testing.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(generated_tokens))\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:71\u001b[0m, in \u001b[0;36mKneserNeyInterpolated.generate\u001b[1;34m(self, context, max_length, seed)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     ngram \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(generated_tokens[\u001b[39m-\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morder \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):])\n\u001b[1;32m---> 71\u001b[0m candidates \u001b[39m=\u001b[39m [(ngram \u001b[39m+\u001b[39m (token,), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob(ngram \u001b[39m+\u001b[39m (token,))) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab]\n\u001b[0;32m     72\u001b[0m candidates\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m candidates:\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     ngram \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(generated_tokens[\u001b[39m-\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morder \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):])\n\u001b[1;32m---> 71\u001b[0m candidates \u001b[39m=\u001b[39m [(ngram \u001b[39m+\u001b[39m (token,), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprob(ngram \u001b[39m+\u001b[39;49m (token,))) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab]\n\u001b[0;32m     72\u001b[0m candidates\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m candidates:\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:50\u001b[0m, in \u001b[0;36mKneserNeyInterpolated.prob\u001b[1;34m(self, ngram)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuation_prob(ngram)\n\u001b[0;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkneser_ney_weight(ngram) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuation_prob(ngram) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprob(ngram[\u001b[39m1\u001b[39;49m:])\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:50\u001b[0m, in \u001b[0;36mKneserNeyInterpolated.prob\u001b[1;34m(self, ngram)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuation_prob(ngram)\n\u001b[0;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkneser_ney_weight(ngram) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontinuation_prob(ngram) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob(ngram[\u001b[39m1\u001b[39m:])\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:40\u001b[0m, in \u001b[0;36mKneserNeyInterpolated.continuation_prob\u001b[1;34m(self, ngram)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m     39\u001b[0m context \u001b[39m=\u001b[39m ngram[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m continuation_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m([ngram \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_counts[order \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m ngram[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m context])\n\u001b[0;32m     41\u001b[0m context_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_counts[order \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m][context] \u001b[39mif\u001b[39;00m order \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m continuation_count \u001b[39m/\u001b[39m context_count\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m     39\u001b[0m context \u001b[39m=\u001b[39m ngram[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m continuation_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m([ngram \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_counts[order \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m ngram[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39m context])\n\u001b[0;32m     41\u001b[0m context_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_counts[order \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m][context] \u001b[39mif\u001b[39;00m order \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m continuation_count \u001b[39m/\u001b[39m context_count\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kni = KneserNeyInterpolated(order=4, discount=0.75)\n",
    "kni.train(tokens=tokens)\n",
    "\n",
    "generated_tokens = kni.generate(context=[\"adam\", \"kilgarriff\"], max_length=20)\n",
    "print(' '.join(generated_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\testing.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Speech-and-Language-Processing/n_gram_models/kneser-ney-interpolation/testing.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m generated_tokens \u001b[39m=\u001b[39m kni\u001b[39m.\u001b[39;49mgenerate(context\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39madam\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mkilgarriff\u001b[39;49m\u001b[39m\"\u001b[39;49m], max_length\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Speech-and-Language-Processing/n_gram_models/kneser-ney-interpolation/testing.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(generated_tokens))\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:71\u001b[0m, in \u001b[0;36mKneserNeyInterpolated.generate\u001b[1;34m(self, context, max_length, seed)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     ngram \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(generated_tokens[\u001b[39m-\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morder \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):])\n\u001b[1;32m---> 71\u001b[0m candidates \u001b[39m=\u001b[39m [(ngram \u001b[39m+\u001b[39m (token,), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob(ngram \u001b[39m+\u001b[39m (token,))) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab]\n\u001b[0;32m     72\u001b[0m candidates\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m candidates:\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     ngram \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(generated_tokens[\u001b[39m-\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39morder \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):])\n\u001b[1;32m---> 71\u001b[0m candidates \u001b[39m=\u001b[39m [(ngram \u001b[39m+\u001b[39m (token,), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprob(ngram \u001b[39m+\u001b[39;49m (token,))) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab]\n\u001b[0;32m     72\u001b[0m candidates\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     74\u001b[0m \u001b[39mif\u001b[39;00m candidates:\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:50\u001b[0m, in \u001b[0;36mKneserNeyInterpolated.prob\u001b[1;34m(self, ngram)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuation_prob(ngram)\n\u001b[0;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkneser_ney_weight(ngram) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuation_prob(ngram) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprob(ngram[\u001b[39m1\u001b[39;49m:])\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:50\u001b[0m, in \u001b[0;36mKneserNeyInterpolated.prob\u001b[1;34m(self, ngram)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontinuation_prob(ngram)\n\u001b[0;32m     49\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkneser_ney_weight(ngram) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontinuation_prob(ngram) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprob(ngram[\u001b[39m1\u001b[39m:])\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:40\u001b[0m, in \u001b[0;36mKneserNeyInterpolated.continuation_prob\u001b[1;34m(self, ngram)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m     39\u001b[0m context \u001b[39m=\u001b[39m ngram[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m continuation_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m([ngram \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_counts[order \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m ngram[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m context])\n\u001b[0;32m     41\u001b[0m context_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_counts[order \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m][context] \u001b[39mif\u001b[39;00m order \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m continuation_count \u001b[39m/\u001b[39m context_count\n",
      "File \u001b[1;32md:\\Speech-and-Language-Processing\\n_gram_models\\kneser-ney-interpolation\\kneser_ney_interpolation.py:40\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m     39\u001b[0m context \u001b[39m=\u001b[39m ngram[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m---> 40\u001b[0m continuation_count \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m([ngram \u001b[39mfor\u001b[39;00m ngram \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram_counts[order \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m ngram[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m] \u001b[39m==\u001b[39m context])\n\u001b[0;32m     41\u001b[0m context_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_counts[order \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m][context] \u001b[39mif\u001b[39;00m order \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n\u001b[0;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m continuation_count \u001b[39m/\u001b[39m context_count\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
